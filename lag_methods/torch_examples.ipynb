{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f266676b",
   "metadata": {},
   "source": [
    "* **Projected Gradient Descent (PGD)**: take a gradient step on $f(x)$, then **project** back to the affine set $\\{x: Ax=b\\}$.\n",
    "* **Equality-Constrained Newton**: solve the **KKT Newton system** each iteration using autograd for $\\nabla f$ and $\\nabla^2 f$.\n",
    "\n",
    "### Problem\n",
    "\n",
    "We’ll use a simple quadratic with a linear equality:\n",
    "\n",
    "$$\n",
    "\\min_x\\; f(x)=\\tfrac12\\|x - v\\|^2 \\quad \\text{s.t.} \\quad Ax=b,\n",
    "$$\n",
    "\n",
    "with $v=(1,2)$, $A=\\begin{bmatrix}1 & 1\\end{bmatrix}$, $b=1$.\n",
    "The exact solution is the Euclidean projection of $v$ onto the line $x_1+x_2=1$, i.e. $x^\\star=(0,1)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e87723cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact solution (by projection of v onto Ax=b):\n",
      "x* = [0. 1.]    f(x*) = 1.0\n",
      "\n",
      "--- Projected Gradient Descent ---\n",
      "[PGD] it=  0  f=5.625000e+00  ||grad||=3.354e+00  ||Ax-b||=0.000e+00\n",
      "[PGD] it= 10  f=1.000005e+00  ||grad||=1.414e+00  ||Ax-b||=0.000e+00\n",
      "[PGD] it= 20  f=1.000000e+00  ||grad||=1.414e+00  ||Ax-b||=0.000e+00\n",
      "[PGD] it= 30  f=1.000000e+00  ||grad||=1.414e+00  ||Ax-b||=0.000e+00\n",
      "[PGD] it= 40  f=1.000000e+00  ||grad||=1.414e+00  ||Ax-b||=0.000e+00\n",
      "[PGD] it= 50  f=1.000000e+00  ||grad||=1.414e+00  ||Ax-b||=0.000e+00\n",
      "[PGD] it= 60  f=1.000000e+00  ||grad||=1.414e+00  ||Ax-b||=0.000e+00\n",
      "[PGD] it= 70  f=1.000000e+00  ||grad||=1.414e+00  ||Ax-b||=0.000e+00\n",
      "[PGD] it= 80  f=1.000000e+00  ||grad||=1.414e+00  ||Ax-b||=0.000e+00\n",
      "[PGD] it= 90  f=1.000000e+00  ||grad||=1.414e+00  ||Ax-b||=0.000e+00\n",
      "[PGD] it=100  f=1.000000e+00  ||grad||=1.414e+00  ||Ax-b||=0.000e+00\n",
      "[PGD] it=110  f=1.000000e+00  ||grad||=1.414e+00  ||Ax-b||=0.000e+00\n",
      "[PGD] it=120  f=1.000000e+00  ||grad||=1.414e+00  ||Ax-b||=0.000e+00\n",
      "[PGD] it=130  f=1.000000e+00  ||grad||=1.414e+00  ||Ax-b||=0.000e+00\n",
      "[PGD] it=140  f=1.000000e+00  ||grad||=1.414e+00  ||Ax-b||=0.000e+00\n",
      "[PGD] it=150  f=1.000000e+00  ||grad||=1.414e+00  ||Ax-b||=0.000e+00\n",
      "[PGD] it=160  f=1.000000e+00  ||grad||=1.414e+00  ||Ax-b||=0.000e+00\n",
      "[PGD] it=170  f=1.000000e+00  ||grad||=1.414e+00  ||Ax-b||=0.000e+00\n",
      "[PGD] it=180  f=1.000000e+00  ||grad||=1.414e+00  ||Ax-b||=0.000e+00\n",
      "[PGD] it=190  f=1.000000e+00  ||grad||=1.414e+00  ||Ax-b||=0.000e+00\n",
      "[PGD] it=199  f=1.000000e+00  ||grad||=1.414e+00  ||Ax-b||=0.000e+00\n",
      "PGD solution: [0. 1.]   f = 1.0   ||Ax-b|| = 0.0\n",
      "\n",
      "--- Equality-Constrained Newton (KKT) ---\n",
      "[Newton] it= 0  f=5.625000e+00  ||rd||=3.354e+00  ||rp||=5.000e-01  step=1.00e+00\n",
      "[Newton] it= 1  f=1.000000e+00  ||rd||=0.000e+00  ||rp||=0.000e+00  step=1.00e+00\n",
      "Newton solution: [0. 1.]   f = 1.0   ||Ax-b|| = 0.0   λ = [1.]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# ---------- Toy instance ----------\n",
    "v = torch.tensor([1.0, 2.0])\n",
    "A = torch.tensor([[1.0, 1.0]])  # shape (m,n) with m=1, n=2\n",
    "b = torch.tensor([1.0])\n",
    "\n",
    "def f(x: torch.Tensor) -> torch.Tensor:\n",
    "    # 0.5 * ||x - v||^2\n",
    "    return 0.5 * torch.sum((x - v) ** 2)\n",
    "\n",
    "# Closed-form projector onto {x : Ax = b}\n",
    "# P(x) = x - A^T (A A^T)^{-1} (A x - b)\n",
    "def project_affine(x: torch.Tensor, A: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    # Assumes A has full row rank (here m=1).\n",
    "    At = A.T\n",
    "    M = A @ At                        # (m,m)\n",
    "    y = torch.linalg.solve(M, (A @ x - b))  # (m,)\n",
    "    return x - At @ y                 # (n,)\n",
    "\n",
    "# ---------- 1) Projected Gradient Descent ----------\n",
    "def projected_gradient_descent(x0, A, b, lr=0.5, max_iter=100, tol=1e-10, verbose=True):\n",
    "    x = x0.detach().clone().requires_grad_(True)\n",
    "    hist = []\n",
    "    for k in range(max_iter):\n",
    "        # grad f(x)\n",
    "        fx = f(x)\n",
    "        (g,) = torch.autograd.grad(fx, x, create_graph=False)\n",
    "        # gradient step\n",
    "        x_new = (x - lr * g).detach()\n",
    "        # projection step\n",
    "        x_new = project_affine(x_new, A, b)\n",
    "        # check convergence (stationarity on the feasible set via gradient norm)\n",
    "        stationarity = g.norm().item()\n",
    "        feas = torch.norm(A @ x_new - b).item()\n",
    "        hist.append((k, fx.item(), stationarity, feas))\n",
    "        if verbose and (k % 10 == 0 or k == max_iter - 1):\n",
    "            print(f\"[PGD] it={k:3d}  f={fx.item():.6e}  ||grad||={stationarity:.3e}  ||Ax-b||={feas:.3e}\")\n",
    "        if stationarity < tol and feas < tol:\n",
    "            x = x_new.detach()\n",
    "            break\n",
    "        x = x_new.detach().requires_grad_(True)\n",
    "    return x.detach(), hist\n",
    "\n",
    "# ---------- 2) Equality-Constrained Newton (KKT Newton) ----------\n",
    "# KKT system for the first-order conditions:\n",
    "# r_d(x, λ) = ∇f(x) + A^T λ = 0           (dual residual)\n",
    "# r_p(x)     = A x - b        = 0         (primal residual)\n",
    "# Newton step solves:\n",
    "# [ H   A^T ] [dx] = -[ r_d ]\n",
    "# [ A    0  ] [dλ]   [ r_p ]\n",
    "#\n",
    "# where H = ∇^2 f(x) (using autograd.hessian for pedagogy).\n",
    "\n",
    "def newton_kkt(x0, A, b, lam0=None, max_iter=20, backtrack=True, rho_merit=1.0, tol=1e-12, verbose=True):\n",
    "    x = x0.detach().clone().requires_grad_(True)\n",
    "    m, n = A.shape\n",
    "    lam = torch.zeros(m) if lam0 is None else lam0.detach().clone()\n",
    "\n",
    "    def grad_f(x):\n",
    "        fx = f(x)\n",
    "        (g,) = torch.autograd.grad(fx, x, create_graph=True)\n",
    "        return g, fx\n",
    "\n",
    "    def hess_f(x):\n",
    "        # Pedagogical: use autograd to build H\n",
    "        return torch.autograd.functional.hessian(lambda z: f(z), x)\n",
    "\n",
    "    def residuals(x, lam):\n",
    "        g, fx = grad_f(x)\n",
    "        rd = g + A.T @ lam               # dual residual\n",
    "        rp = A @ x - b                   # primal residual\n",
    "        return rd, rp, fx\n",
    "\n",
    "    def merit(x, lam):\n",
    "        # A simple merit function to drive line search: phi = f(x) + (rho/2)||Ax-b||^2\n",
    "        rd, rp, fx = residuals(x, lam)\n",
    "        return fx + 0.5 * rho_merit * torch.sum(rp * rp)\n",
    "\n",
    "    hist = []\n",
    "    for k in range(max_iter):\n",
    "        rd, rp, fx = residuals(x, lam)\n",
    "        H = hess_f(x)\n",
    "\n",
    "        # Build and solve the KKT linear system\n",
    "        KKT = torch.zeros((n + m, n + m), dtype=x.dtype)\n",
    "        KKT[:n, :n] = H\n",
    "        KKT[:n, n:] = A.T\n",
    "        KKT[n:, :n] = A\n",
    "\n",
    "        rhs = torch.cat([-rd, -rp], dim=0)\n",
    "        sol = torch.linalg.solve(KKT, rhs)\n",
    "        dx = sol[:n]\n",
    "        dlam = sol[n:]\n",
    "\n",
    "        # Backtracking line search on merit function (optional; helps outside quadratics)\n",
    "        t = 1.0\n",
    "        phi0 = merit(x, lam)\n",
    "        if backtrack:\n",
    "            # Simple Armijo-like decrease on the merit function\n",
    "            c, beta = 1e-4, 0.5\n",
    "            # directional derivative of merit ~ grad_phi^T [dx; rp-part], approximated numerically is fine for teaching\n",
    "            while True:\n",
    "                x_try = (x + t * dx).detach().requires_grad_(True)\n",
    "                lam_try = lam + t * dlam\n",
    "                phi_try = merit(x_try, lam_try)\n",
    "                if phi_try <= phi0 - c * t * (rd.norm()**2 + rp.norm()**2):\n",
    "                    break\n",
    "                t *= beta\n",
    "                if t < 1e-12:\n",
    "                    break\n",
    "\n",
    "        # Update\n",
    "        x = (x + t * dx).detach().requires_grad_(True)\n",
    "        lam = (lam + t * dlam).detach()\n",
    "\n",
    "        # Logging\n",
    "        res_norm = torch.sqrt(rd.norm()**2 + rp.norm()**2).item()\n",
    "        hist.append((k, fx.item(), rd.norm().item(), rp.norm().item(), res_norm, t))\n",
    "        if verbose:\n",
    "            print(f\"[Newton] it={k:2d}  f={fx.item():.6e}  ||rd||={rd.norm().item():.3e}  ||rp||={rp.norm().item():.3e}  step={t:.2e}\")\n",
    "        if res_norm < tol:\n",
    "            break\n",
    "\n",
    "    return x.detach(), lam.detach(), hist\n",
    "\n",
    "# ---------- Run both methods ----------\n",
    "x0 = torch.tensor([2.5, -1.0])  # arbitrary infeasible start\n",
    "\n",
    "print(\"Exact solution (by projection of v onto Ax=b):\")\n",
    "x_star = project_affine(v, A, b)\n",
    "print(\"x* =\", x_star.numpy(), \"   f(x*) =\", f(x_star).item())\n",
    "\n",
    "print(\"\\n--- Projected Gradient Descent ---\")\n",
    "x_pgd, pgd_hist = projected_gradient_descent(x0, A, b, lr=0.5, max_iter=200, tol=1e-12, verbose=True)\n",
    "print(\"PGD solution:\", x_pgd.numpy(), \"  f =\", f(x_pgd).item(), \"  ||Ax-b|| =\", torch.norm(A @ x_pgd - b).item())\n",
    "\n",
    "print(\"\\n--- Equality-Constrained Newton (KKT) ---\")\n",
    "x_nt, lam_nt, nt_hist = newton_kkt(x0, A, b, max_iter=10, verbose=True)\n",
    "print(\"Newton solution:\", x_nt.numpy(), \"  f =\", f(x_nt).item(), \"  ||Ax-b|| =\", torch.norm(A @ x_nt - b).item(), \"  λ =\", lam_nt.numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
