{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84dc8c43",
   "metadata": {},
   "source": [
    "# Lagrangian and Augmented Lagrangian Methods\n",
    "\n",
    "## Chapter 1: The Challenge of Constrained Optimization\n",
    "\n",
    "### Why Constraints Make Everything Harder\n",
    "\n",
    "Imagine you're trying to find the lowest point in a hilly landscape while blindfolded. In **unconstrained optimization**, you can simply follow the steepest downward slope (negative gradient) until you reach a valley.\n",
    "\n",
    "Now imagine there are walls, rivers, and forbidden zones you cannot cross. This is **constrained optimization** - you must find the best solution while respecting rules (constraints). The steepest descent might lead you into a wall!\n",
    "\n",
    "### A Simple Visual Example\n",
    "\n",
    "Let's start with something you can visualize:\n",
    "\n",
    "**Problem**: Minimize the distance from the origin: $f(x_1, x_2) = x_1^2 + x_2^2$\n",
    "**Constraint**: You must stay on the line $x_1 + x_2 = 2$\n",
    "\n",
    "```\n",
    "Without constraint: optimal point is (0,0) with f* = 0\n",
    "With constraint: you're forced to stay on the line x₁ + x₂ = 2\n",
    "```\n",
    "\n",
    "**Intuitive Solution Process**:\n",
    "1. Draw the line $x_1 + x_2 = 2$ \n",
    "2. Draw circles centered at origin: $x_1^2 + x_2^2 = c$ for various values of $c$\n",
    "3. Find the smallest circle that still touches the line\n",
    "4. The touching point is your optimal solution: $(1, 1)$ with $f^* = 2$\n",
    "\n",
    "**Key Insight**: At the optimal point, the constraint boundary and an objective function contour are **tangent** - they touch but don't cross.\n",
    "\n",
    "### Types of Optimization Problems\n",
    "\n",
    "We'll build up complexity gradually:\n",
    "\n",
    "**Level 1: Unconstrained**\n",
    "$$\\min_{x} f(x)$$\n",
    "*Solution*: Set $\\nabla f(x) = 0$ and solve.\n",
    "\n",
    "**Level 2: Equality Constraints Only**\n",
    "$$\\min_{x} f(x) \\quad \\text{subject to} \\quad g(x) = 0$$\n",
    "*New challenge*: Can't just set gradient to zero - might violate constraint!\n",
    "\n",
    "**Level 3: Mixed Constraints**  \n",
    "$$\\min_{x} f(x) \\quad \\text{subject to} \\quad g(x) = 0, \\quad h(x) \\leq 0$$\n",
    "*Added complexity*: Some constraints might not be active (binding) at the optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cef7cfb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb544769",
   "metadata": {},
   "source": [
    "## Chapter 2: The Lagrangian - Turning Constraints into Costs\n",
    "\n",
    "### The Big Idea: Shadow Prices\n",
    "\n",
    "Imagine you're managing a factory with production constraints. What if you could \"buy your way out\" of each constraint? How much would you be willing to pay?\n",
    "\n",
    "- If relaxing constraint $i$ by one unit would save you $100 in objective cost, that constraint has a **shadow price** of $100\n",
    "- If a constraint isn't limiting you (inactive), its shadow price is $0\n",
    "- These shadow prices are exactly what **Lagrange multipliers** represent!\n",
    "\n",
    "### The Lagrangian Function: Mathematical Form\n",
    "\n",
    "For the equality-constrained problem:\n",
    "$$\\min_{x} f(x) \\quad \\text{subject to} \\quad g_i(x) = 0, \\quad i = 1,\\ldots,m$$\n",
    "\n",
    "The **Lagrangian** incorporates constraints as weighted penalty terms:\n",
    "$$L(x, \\lambda) = f(x) + \\sum_{i=1}^m \\lambda_i g_i(x)$$\n",
    "\n",
    "**What each piece means**:\n",
    "- $f(x)$: your original objective\n",
    "- $\\lambda_i$: shadow price (Lagrange multiplier) for constraint $i$  \n",
    "- $g_i(x)$: constraint violation (zero when satisfied)\n",
    "- $\\lambda_i g_i(x)$: cost of violating constraint $i$\n",
    "\n",
    "### Step-by-Step Solution Process\n",
    "\n",
    "**The Method of Lagrange Multipliers**:\n",
    "\n",
    "1. **Set up**: Form $L(x, \\lambda) = f(x) + \\sum_{i=1}^m \\lambda_i g_i(x)$\n",
    "\n",
    "2. **Take derivatives**: \n",
    "   - $\\frac{\\partial L}{\\partial x_j} = 0$ for all $j$ (stationarity)\n",
    "   - $\\frac{\\partial L}{\\partial \\lambda_i} = g_i(x) = 0$ for all $i$ (feasibility)\n",
    "\n",
    "3. **Solve the system**: This gives you $(n + m)$ equations in $(n + m)$ unknowns\n",
    "\n",
    "4. **Check second-order conditions** (for multiple solutions)\n",
    "\n",
    "### Worked Example: Minimizing Distance to Origin\n",
    "\n",
    "**Problem**: $\\min_{x_1, x_2} x_1^2 + x_2^2$ subject to $x_1 + x_2 - 2 = 0$\n",
    "\n",
    "**Step 1**: Form Lagrangian\n",
    "$$L(x_1, x_2, \\lambda) = x_1^2 + x_2^2 + \\lambda(x_1 + x_2 - 2)$$\n",
    "\n",
    "**Step 2**: Take derivatives and set to zero\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial x_1} = 2x_1 + \\lambda = 0 \\Rightarrow x_1 = -\\frac{\\lambda}{2}$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_2} = 2x_2 + \\lambda = 0 \\Rightarrow x_2 = -\\frac{\\lambda}{2}\n",
    "$$  \n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\lambda} = x_1 + x_2 - 2 = 0 \\Rightarrow x_1 + x_2 = 2$$\n",
    "\n",
    "**Step 3**: Solve the system\n",
    "Substituting: $-\\frac{\\lambda}{2} - \\frac{\\lambda}{2} = 2 \\Rightarrow -\\lambda = 2 \\Rightarrow \\lambda = -2$\n",
    "\n",
    "Therefore: $x_1^* = x_2^* = 1$ and $f^* = 2$\n",
    "\n",
    "**Step 4**: Interpret the multiplier\n",
    "$\\lambda^* = -2$ means: if we relaxed the constraint from $x_1 + x_2 = 2$ to $x_1 + x_2 = 2 + \\epsilon$, our objective would decrease by approximately $2\\epsilon$ for small $\\epsilon > 0$.\n",
    "\n",
    "### Geometric Interpretation: Why This Works\n",
    "\n",
    "At the optimal point, two crucial vectors are **parallel**:\n",
    "1. Gradient of objective function: $\\nabla f(x^*)$\n",
    "2. Gradient of constraint: $\\nabla g(x^*)$\n",
    "\n",
    "Mathematically: $\\nabla f(x^*) = -\\lambda^* \\nabla g(x^*)$\n",
    "\n",
    "**Why?** If they weren't parallel, you could move along the constraint boundary in a direction that decreases the objective - meaning you're not optimal!\n",
    "\n",
    "**Visual intuition**: \n",
    "- Objective function contours are like elevation lines on a topographic map\n",
    "- Constraint is like a hiking trail you must stay on\n",
    "- At the optimum, the trail is tangent to an elevation contour\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e70722",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b507a6f",
   "metadata": {},
   "source": [
    "\n",
    "## Chapter 3: Handling Inequality Constraints - The KKT Conditions\n",
    "\n",
    "### The New Challenge: Sometimes Constraints Don't Matter\n",
    "\n",
    "With inequality constraints $h_j(x) \\leq 0$, we face a dilemma:\n",
    "- If $h_j(x^*) < 0$ (constraint inactive): it doesn't affect the optimum\n",
    "- If $h_j(x^*) = 0$ (constraint active): it acts like an equality constraint\n",
    "\n",
    "But we don't know in advance which constraints will be active!\n",
    "\n",
    "### The Karush-Kuhn-Tucker (KKT) Approach\n",
    "\n",
    "The **KKT conditions** elegantly handle this uncertainty:\n",
    "\n",
    "For the problem:\n",
    "$$\\min_{x} f(x) \\quad \\text{s.t.} \\quad g_i(x) = 0, \\quad h_j(x) \\leq 0$$\n",
    "\n",
    "The Lagrangian becomes:\n",
    "$$L(x, \\lambda, \\mu) = f(x) + \\sum_{i=1}^m \\lambda_i g_i(x) + \\sum_{j=1}^p \\mu_j h_j(x)$$\n",
    "\n",
    "**KKT Conditions** (necessary for optimality):\n",
    "1. **Stationarity**: $\\nabla_x L(x^*, \\lambda^*, \\mu^*) = 0$\n",
    "2. **Primal feasibility**: $g_i(x^*) = 0$, $h_j(x^*) \\leq 0$  \n",
    "3. **Dual feasibility**: $\\mu_j^* \\geq 0$\n",
    "4. **Complementary slackness**: $\\mu_j^* h_j(x^*) = 0$\n",
    "\n",
    "### Understanding Complementary Slackness\n",
    "\n",
    "This is the trickiest condition. It means **exactly one** of these must be true:\n",
    "- $\\mu_j^* = 0$ (multiplier is zero): constraint $j$ is inactive  \n",
    "- $h_j(x^*) = 0$ (constraint is active): constraint $j$ affects the optimum\n",
    "\n",
    "**Economic interpretation**: You either:\n",
    "- Don't value relaxing the constraint ($\\mu_j^* = 0$) because it's not limiting you\n",
    "- Are limited by the constraint ($h_j(x^*) = 0$) and would pay to relax it ($\\mu_j^* > 0$)\n",
    "\n",
    "### Worked Example: Production Planning\n",
    "\n",
    "**Problem**: A factory makes widgets to maximize profit\n",
    "$$\\max_{x} 3x \\quad \\text{subject to} \\quad x \\leq 100 \\quad \\text{(capacity)}, \\quad x \\geq 0$$\n",
    "\n",
    "Converting to minimization: $\\min_{x} -3x$ subject to $x - 100 \\leq 0$, $-x \\leq 0$\n",
    "\n",
    "**Lagrangian**: $L(x, \\mu_1, \\mu_2) = -3x + \\mu_1(x - 100) + \\mu_2(-x)$\n",
    "\n",
    "**KKT conditions**:\n",
    "1. $\\frac{\\partial L}{\\partial x} = -3 + \\mu_1 - \\mu_2 = 0$\n",
    "2. $x - 100 \\leq 0$, $-x \\leq 0$  \n",
    "3. $\\mu_1, \\mu_2 \\geq 0$\n",
    "4. $\\mu_1(x - 100) = 0$, $\\mu_2(-x) = 0$\n",
    "\n",
    "**Solution strategy**: Try different cases for which constraints are active.\n",
    "\n",
    "**Case 1**: Both constraints inactive ($x \\in (0, 100)$)\n",
    "Then $\\mu_1 = \\mu_2 = 0$, so $-3 = 0$ (impossible!)\n",
    "\n",
    "**Case 2**: Only capacity constraint active ($x = 100$)\n",
    "Then $\\mu_2 = 0$ and $-3 + \\mu_1 = 0$, so $\\mu_1 = 3 \\geq 0$ ✓\n",
    "\n",
    "**Answer**: $x^* = 100$, $\\mu_1^* = 3$, $\\mu_2^* = 0$\n",
    "\n",
    "**Interpretation**: Produce at full capacity. The shadow price of capacity is $3 - you'd pay up to $3 per unit of additional capacity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5948e9a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df979643",
   "metadata": {},
   "source": [
    "\n",
    "## Chapter 4: Why Classical Methods Sometimes Fail\n",
    "\n",
    "### The Conditioning Problem\n",
    "\n",
    "Classical Lagrangian methods solve the KKT system:\n",
    "$$\\begin{bmatrix} \\nabla^2_x L & \\nabla g^T \\\\ \\nabla g & 0 \\end{bmatrix} \\begin{bmatrix} \\Delta x \\\\ \\Delta \\lambda \\end{bmatrix} = \\begin{bmatrix} -\\nabla_x L \\\\ -g \\end{bmatrix}$$\n",
    "\n",
    "**Problems arise when**:\n",
    "- Constraints are nearly dependent (rank deficient)\n",
    "- Hessian is ill-conditioned  \n",
    "- Poor multiplier estimates cause slow convergence\n",
    "\n",
    "### A Motivating Example: Nearly Parallel Constraints\n",
    "\n",
    "Consider:\n",
    "$$\\min_{x_1, x_2} x_1^2 + x_2^2 \\quad \\text{s.t.} \\quad x_1 + x_2 = 1, \\quad x_1 + x_2 + \\epsilon = 1$$\n",
    "\n",
    "As $\\epsilon \\to 0$, the constraints become identical, making the KKT matrix singular.\n",
    "\n",
    "**Classical method**: Fails or converges very slowly  \n",
    "**Augmented Lagrangian**: Remains robust!\n",
    "\n",
    "### The Penalty Method Alternative\n",
    "\n",
    "**Pure penalty approach**:\n",
    "$$\\min_x f(x) + \\frac{c}{2}\\sum_{i=1}^m g_i(x)^2$$\n",
    "\n",
    "**Problems**:\n",
    "- Need $c \\to \\infty$ for exact constraint satisfaction\n",
    "- Becomes numerically ill-conditioned as $c$ increases\n",
    "- No natural multiplier estimates\n",
    "\n",
    "**The key insight**: What if we combine the best of both approaches?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c312a56",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6911d06",
   "metadata": {},
   "source": [
    "## Chapter 5: Augmented Lagrangian Methods - The Best of Both Worlds\n",
    "\n",
    "### The Brilliant Idea\n",
    "\n",
    "The **augmented Lagrangian** combines:\n",
    "- **Linear terms** $\\lambda_i g_i(x)$ (give correct gradient information)\n",
    "- **Quadratic penalties** (drive violations to zero)\n",
    "\n",
    "But here's where it gets interesting - **equality and inequality constraints need different treatment**!\n",
    "\n",
    "### Handling Equality Constraints: The Simple Case\n",
    "\n",
    "For equality constraints $g_i(x) = 0$, the augmented Lagrangian is straightforward:\n",
    "\n",
    "$L_A(x, \\lambda, c) = f(x) + \\sum_{i=1}^m \\lambda_i g_i(x) + \\frac{c}{2} \\sum_{i=1}^m g_i(x)^2$\n",
    "\n",
    "**Why this works for equalities**:\n",
    "- Constraint violation $g_i(x) \\neq 0$ is always \"bad\" \n",
    "- Quadratic penalty $\\frac{c}{2}g_i(x)^2$ is always positive when violated\n",
    "- Linear term $\\lambda_i g_i(x)$ provides correct gradient direction\n",
    "\n",
    "### Handling Inequality Constraints: The Tricky Part\n",
    "\n",
    "For inequality constraints $h_j(x) \\leq 0$, we need to be more careful. We only want to penalize **violations** ($h_j(x) > 0$), not when the constraint is satisfied ($h_j(x) \\leq 0$).\n",
    "\n",
    "**The key insight**: Use a modified penalty that \"turns off\" when constraints are satisfied.\n",
    "\n",
    "#### Option 1: The Classic Approach (Powell-Hestenes-Rockafellar)\n",
    "\n",
    "$\\phi_j(x, \\mu_j, c) = \\begin{cases}\n",
    "\\mu_j h_j(x) + \\frac{c}{2}h_j(x)^2 & \\text{if } \\mu_j + c h_j(x) \\geq 0 \\\\\n",
    "-\\frac{\\mu_j^2}{2c} & \\text{if } \\mu_j + c h_j(x) < 0\n",
    "\\end{cases}$\n",
    "\n",
    "**What this means**:\n",
    "- **When constraint is violated** ($h_j(x) > 0$): Apply both linear and quadratic penalty\n",
    "- **When constraint is well-satisfied**: Only apply a constant correction term\n",
    "- **The switching condition** $\\mu_j + c h_j(x) \\geq 0$ automatically handles the transition\n",
    "\n",
    "#### Option 2: The Simpler Max-Function Approach\n",
    "\n",
    "Many implementations use this more intuitive form:\n",
    "\n",
    "$\\phi_j(x, \\mu_j, c) = \\mu_j h_j(x) + \\frac{c}{2}[\\max(0, h_j(x) + \\mu_j/c)]^2 - \\frac{\\mu_j^2}{2c}$\n",
    "\n",
    "**Intuitive explanation**:\n",
    "- $\\max(0, h_j(x) + \\mu_j/c)$ is zero when constraint is well-satisfied\n",
    "- Only apply quadratic penalty when there's a \"shifted violation\"\n",
    "- The shift $\\mu_j/c$ accounts for the current multiplier estimate\n",
    "\n",
    "#### The Complete Augmented Lagrangian\n",
    "\n",
    "For the general problem with both constraint types:\n",
    "$\\min_{x} f(x) \\quad \\text{s.t.} \\quad g_i(x) = 0, \\quad h_j(x) \\leq 0$\n",
    "\n",
    "The **full augmented Lagrangian** is:\n",
    "$L_A(x, \\lambda, \\mu, c) = f(x) + \\sum_{i=1}^m \\left[\\lambda_i g_i(x) + \\frac{c}{2} g_i(x)^2\\right] + \\sum_{j=1}^p \\phi_j(x, \\mu_j, c)$\n",
    "\n",
    "**Magic property**: Unlike pure penalty methods, $c$ doesn't need to go to infinity!\n",
    "\n",
    "### Why This Works: The Finite Penalty Property\n",
    "\n",
    "**Theorem**: Under mild conditions, there exists a finite $\\bar{c}$ such that for all $c \\geq \\bar{c}$:\n",
    "$$x^*(c) = \\arg\\min_x L_A(x, \\lambda^*, c)$$\n",
    "exactly satisfies all constraints and is the true optimum.\n",
    "\n",
    "**What this means**: Once $c$ is large enough, further increases don't change the solution - we get exact constraint satisfaction with finite penalty parameter.\n",
    "\n",
    "### Different Multiplier Updates for Different Constraint Types\n",
    "\n",
    "The multiplier updates also differ between equality and inequality constraints:\n",
    "\n",
    "#### For Equality Constraints:\n",
    "$\\lambda_i^{k+1} = \\lambda_i^k + c^k g_i(x^{k+1})$\n",
    "\n",
    "**Simple and symmetric**: Always update in the direction of constraint violation.\n",
    "\n",
    "#### For Inequality Constraints:\n",
    "$\\mu_j^{k+1} = \\max\\left(0, \\mu_j^k + c^k h_j(x^{k+1})\\right)$\n",
    "\n",
    "**Key differences**:\n",
    "- **Max with zero**: Ensures $\\mu_j \\geq 0$ (dual feasibility)\n",
    "- **Can become zero**: If constraint becomes inactive, multiplier automatically goes to zero\n",
    "- **Automatic switching**: The algorithm naturally determines which constraints are active\n",
    "\n",
    "#### Why These Updates Make Sense\n",
    "\n",
    "**For equalities**: \n",
    "- Violation in either direction is bad\n",
    "- Multiplier can be positive or negative\n",
    "- Always update toward zero violation\n",
    "\n",
    "**For inequalities**:\n",
    "- Only positive violations ($h_j(x) > 0$) are bad\n",
    "- Multiplier must stay non-negative  \n",
    "- If constraint becomes slack ($h_j(x) < 0$), multiplier should go to zero\n",
    "\n",
    "### The Method of Multipliers Algorithm (Complete Version)\n",
    "\n",
    "```\n",
    "Algorithm: Method of Multipliers (Equality + Inequality)\n",
    "Input: x⁰, λ⁰, μ⁰, c⁰ > 0, τ > 1, εₖ > 0\n",
    "k = 0\n",
    "\n",
    "While not converged:\n",
    "    1. Approximately solve: x^(k+1) ≈ argmin L_A(x, λᵏ, μᵏ, cᵏ)\n",
    "       (stop when ||∇ₓL_A|| ≤ εₖ)\n",
    "    \n",
    "    2. Update multipliers:  \n",
    "       For i = 1, ..., m:\n",
    "           λᵢ^(k+1) = λᵢᵏ + cᵏ gᵢ(x^(k+1))\n",
    "       \n",
    "       For j = 1, ..., p:\n",
    "           μⱼ^(k+1) = max(0, μⱼᵏ + cᵏ hⱼ(x^(k+1)))\n",
    "    \n",
    "    3. Check convergence:\n",
    "       If max|gᵢ(x^(k+1))| ≤ ε_eq AND max{hⱼ(x^(k+1))} ≤ ε_ineq: STOP\n",
    "    \n",
    "    4. Update penalty parameter:\n",
    "       constraint_violation = max(max|gᵢ(x^(k+1))|, max{0, hⱼ(x^(k+1))})\n",
    "       If constraint_violation > 0.25 * previous_violation:\n",
    "           cᵏ⁺¹ = τ * cᵏ\n",
    "       Else:\n",
    "           cᵏ⁺¹ = cᵏ\n",
    "    \n",
    "    5. k = k + 1\n",
    "```\n",
    "\n",
    "### Understanding the Multiplier Update\n",
    "\n",
    "The multiplier update rule:\n",
    "$$\\lambda_i^{k+1} = \\lambda_i^k + c^k g_i(x^{k+1})$$\n",
    "\n",
    "**Intuitive explanation**:\n",
    "- If $g_i(x^{k+1}) > 0$ (constraint violated): increase $\\lambda_i$ to penalize more\n",
    "- If $g_i(x^{k+1}) < 0$ (constraint over-satisfied): decrease $\\lambda_i$  \n",
    "- If $g_i(x^{k+1}) = 0$ (constraint satisfied): keep $\\lambda_i$ unchanged\n",
    "\n",
    "This is actually a **gradient ascent** step on the dual function!\n",
    "\n",
    "### Worked Example: Portfolio Optimization with Mixed Constraints\n",
    "\n",
    "Let's see how augmented Lagrangian handles both constraint types in practice.\n",
    "\n",
    "**Problem**: Minimize portfolio risk while achieving target return\n",
    "$\\begin{align}\n",
    "\\min_{w} \\quad & \\frac{1}{2} w^T Q w \\\\\n",
    "\\text{s.t.} \\quad & \\mathbf{1}^T w = 1 & \\text{(budget: equality)} \\\\\n",
    "& \\mu^T w = r_{\\text{target}} & \\text{(return: equality)} \\\\\n",
    "& w_i \\geq 0 \\quad \\forall i & \\text{(no short selling: inequality)}\n",
    "\\end{align}$\n",
    "\n",
    "where $w$ are portfolio weights, $Q$ is covariance matrix, $\\mu$ are expected returns.\n",
    "\n",
    "Converting inequalities: $w_i \\geq 0 \\Rightarrow -w_i \\leq 0$\n",
    "\n",
    "**Augmented Lagrangian**:\n",
    "$L_A = \\frac{1}{2} w^T Q w + \\lambda_1(\\mathbf{1}^T w - 1) + \\frac{c}{2}(\\mathbf{1}^T w - 1)^2$\n",
    "$+ \\lambda_2(\\mu^T w - r_{\\text{target}}) + \\frac{c}{2}(\\mu^T w - r_{\\text{target}})^2$\n",
    "$+ \\sum_{i=1}^n \\phi_i(-w_i, \\nu_i, c)$\n",
    "\n",
    "**Step-by-step iteration**:\n",
    "\n",
    "**Iteration 1**: Start with $\\lambda_1^0 = \\lambda_2^0 = 0$, $\\nu_i^0 = 0$, $c^0 = 1$\n",
    "\n",
    "1. **Solve subproblem**: Minimize $L_A$ w.r.t. $w$ (quadratic program)\n",
    "   Result: $w^1 = [0.4, 0.8, -0.2]$ (violates non-negativity!)\n",
    "\n",
    "2. **Update equality multipliers**:\n",
    "   - Budget violation: $\\mathbf{1}^T w^1 - 1 = 1.0$\n",
    "   - $\\lambda_1^1 = 0 + 1 \\cdot 1.0 = 1.0$\n",
    "   - Return violation: $\\mu^T w^1 - r_{\\text{target}} = 0.05$  \n",
    "   - $\\lambda_2^1 = 0 + 1 \\cdot 0.05 = 0.05$\n",
    "\n",
    "3. **Update inequality multipliers**:\n",
    "   - For $w_1 = 0.4$: $\\nu_1^1 = \\max(0, 0 + 1 \\cdot (-0.4)) = 0$\n",
    "   - For $w_2 = 0.8$: $\\nu_2^1 = \\max(0, 0 + 1 \\cdot (-0.8)) = 0$  \n",
    "   - For $w_3 = -0.2$: $\\nu_3^1 = \\max(0, 0 + 1 \\cdot (0.2)) = 0.2$\n",
    "\n",
    "4. **Check convergence**: Large violations, continue with $c^1 = 2$\n",
    "\n",
    "**Iteration 2**: With updated multipliers and penalty parameter\n",
    "\n",
    "1. **Solve subproblem**: Now the penalty strongly discourages negative $w_3$\n",
    "   Result: $w^2 = [0.35, 0.65, 0.0]$ (much better!)\n",
    "\n",
    "2. **Update multipliers**: Smaller violations lead to smaller updates\n",
    "\n",
    "**Key observations**:\n",
    "- **Equality constraints**: Always get linear + quadratic penalty\n",
    "- **Inequality constraints**: Only $w_3$ gets penalized (it was violated)  \n",
    "- **Active/inactive detection**: $\\nu_3$ becomes positive, others stay zero\n",
    "- **Automatic adaptation**: Algorithm learns which constraints matter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2e432e",
   "metadata": {},
   "source": [
    "# Step-by-Step Guide: Inequality Constraints in Augmented Lagrangian\n",
    "\n",
    "## Overview\n",
    "We want to understand how inequality constraints $g(x) \\leq 0$ are handled in augmented Lagrangian methods. We'll build this understanding in small, clear steps.\n",
    "\n",
    "## Step 1: Understanding the Problem\n",
    "**What we start with:**\n",
    "- Optimization problem: minimize $f(x)$ subject to $g(x) \\leq 0$\n",
    "- The constraint $g(x) \\leq 0$ means \"don't exceed the limit\"\n",
    "- Unlike equality constraints, this doesn't need to be exactly satisfied—we just can't violate it\n",
    "\n",
    "**Key insight:** Inequality constraints are fundamentally different from equality constraints because they allow \"slack\" or \"unused capacity.\"\n",
    "\n",
    "## Step 2: The Slack Variable Transformation\n",
    "**The Big Idea:** Convert the inequality into an equality by introducing a \"slack\" variable.\n",
    "\n",
    "**Step 2a: Introduce the slack variable**\n",
    "- Add a new variable $s \\geq 0$ \n",
    "- Transform: $g(x) \\leq 0$ becomes $g(x) + s = 0$ with $s \\geq 0$\n",
    "\n",
    "**Step 2b: Interpret the slack variable**\n",
    "- If $s > 0$: constraint is loose (we're inside the feasible region)\n",
    "- If $s = 0$: constraint is tight (we're exactly at the boundary)\n",
    "- $s$ literally represents \"unused room\" in the constraint\n",
    "\n",
    "**Concrete Example:**\n",
    "Budget constraint: $x_1 + 2x_2 \\leq 10$\n",
    "- If we spend $x_1 + 2x_2 = 7$, then $s = 3$ (we have $3 left unused)\n",
    "- If we spend $x_1 + 2x_2 = 10$, then $s = 0$ (budget is fully used)\n",
    "\n",
    "## Step 3: Setting Up the Augmented Lagrangian (Naive Form)\n",
    "**Now we have an equality constraint:** $g(x) + s = 0$ with $s \\geq 0$\n",
    "\n",
    "**Step 3a: Write the augmented Lagrangian**\n",
    "$$\\mathcal{L}_A(x,s,\\mu,\\rho) = f(x) + \\mu(g(x) + s) + \\frac{\\rho}{2}(g(x) + s)^2$$\n",
    "subject to $s \\geq 0$\n",
    "\n",
    "**Step 3b: Understand the terms**\n",
    "- $f(x)$: original objective\n",
    "- $\\mu(g(x) + s)$: Lagrange multiplier term (pushes toward constraint satisfaction)\n",
    "- $\\frac{\\rho}{2}(g(x) + s)^2$: penalty term (quadratic penalty for constraint violation)\n",
    "\n",
    "## Step 4: The Problem with Extra Variables\n",
    "**Issue:** Carrying the slack variable $s$ increases the problem size\n",
    "- More variables to optimize over\n",
    "- More computational cost\n",
    "- Can we eliminate $s$ somehow?\n",
    "\n",
    "**The key insight:** For a fixed $x$, $\\mu$, and $\\rho$, we can find the optimal $s$ analytically!\n",
    "\n",
    "## Step 5: Eliminating the Slack Variable\n",
    "**Step 5a: Find optimal slack for fixed $(x, \\mu, \\rho)$**\n",
    "\n",
    "We minimize $\\mathcal{L}_A$ over $s \\geq 0$:\n",
    "$$\\min_{s \\geq 0} \\left[ \\mu s + \\frac{\\rho}{2}(g(x) + s)^2 \\right]$$\n",
    "\n",
    "Taking the derivative with respect to $s$:\n",
    "$$\\frac{d}{ds} = \\mu + \\rho(g(x) + s) = 0$$\n",
    "\n",
    "This gives us: $s = -g(x) - \\frac{\\mu}{\\rho}$\n",
    "\n",
    "**Step 5b: Apply the nonnegativity constraint**\n",
    "Since $s \\geq 0$, the optimal slack is:\n",
    "$$s^* = \\max\\left\\{0, -g(x) - \\frac{\\mu}{\\rho}\\right\\}$$\n",
    "\n",
    "**Step 5c: Interpret the cases**\n",
    "- If $g(x) + \\frac{\\mu}{\\rho} \\leq 0$: then $s^* = -g(x) - \\frac{\\mu}{\\rho} > 0$ (constraint is loose)\n",
    "- If $g(x) + \\frac{\\mu}{\\rho} > 0$: then $s^* = 0$ (constraint is tight or violated)\n",
    "\n",
    "## Step 6: The Final Compact Form\n",
    "**Step 6a: Substitute back**\n",
    "Plug $s^*$ back into the augmented Lagrangian and simplify.\n",
    "\n",
    "After algebraic manipulation (expanding the quadratic and simplifying), we get:\n",
    "$$\\mathcal{L}_A(x,\\mu,\\rho) = f(x) + \\frac{\\max(0, \\mu + \\rho g(x))^2 - \\mu^2}{2\\rho}$$\n",
    "\n",
    "**Step 6b: Verify the intuition**\n",
    "- When $\\mu + \\rho g(x) \\leq 0$: the max is 0, so we get $f(x) - \\frac{\\mu^2}{2\\rho}$\n",
    "- When $\\mu + \\rho g(x) > 0$: the max is $\\mu + \\rho g(x)$, creating a penalty\n",
    "\n",
    "\n",
    "## Step 7: Connection to Implementation\n",
    "**This compact form is exactly what modern implementations use!**\n",
    "\n",
    "In your PyTorch code, you would see something like:\n",
    "```python\n",
    "penalty_term = (torch.clamp(mu + rho * g_x, min=0)**2 - mu**2) / (2 * rho)\n",
    "augmented_lagrangian = objective + penalty_term\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- No extra slack variables to track\n",
    "- Efficient computation using clamp/max operations\n",
    "- Automatically handles both loose and tight constraints\n",
    "- Scales well to many inequality constraints\n",
    "\n",
    "## Summary: The Journey\n",
    "1. **Started with:** Inequality constraint $g(x) \\leq 0$\n",
    "2. **Added slack:** Transform to $g(x) + s = 0$, $s \\geq 0$\n",
    "3. **Built augmented Lagrangian:** With both $x$ and $s$ as variables\n",
    "4. **Eliminated slack:** Found optimal $s^*$ analytically\n",
    "5. **Got compact form:** Final expression with only $x$ as variable\n",
    "6. **Connected to code:** This is what implementations actually use\n",
    "\n",
    "The key insight is that the \"slack variable trick\" helps us understand the intuition, but we can eliminate the extra variables to get an efficient computational form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49d9b56",
   "metadata": {},
   "source": [
    "# Convergence Theory for Augmented Lagrangian Methods\n",
    "\n",
    "## 1. Why Other Methods Struggle\n",
    "\n",
    "Think of constrained optimization as trying to balance on a narrow ridge:\n",
    "\n",
    "* **Pure penalty methods**: Imagine punishing deviations from the ridge with a giant hammer. To really stay on the ridge, you need an infinitely heavy hammer ($\\rho \\to \\infty$). But then walking becomes impossible — every step requires huge effort (ill-conditioning).\n",
    "\n",
    "* **Pure Lagrangian methods**: Here you walk with no hammer, just a rope pulling you towards the ridge (multipliers). But the rope pulls both ways — sometimes stabilizing, sometimes destabilizing. Mathematically: indefinite Hessians → unstable saddle point problem.\n",
    "\n",
    "## 2. The Augmented Lagrangian \"Sweet Spot\"\n",
    "\n",
    "The genius idea: **use a rope and a light hammer together.**\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_A(x,\\lambda,\\rho) = f(x) + \\lambda^\\top h(x) + \\tfrac{\\rho}{2}\\|h(x)\\|^2\n",
    "$$\n",
    "\n",
    "* **Rope (multipliers $\\lambda$)**: encode constraint forces, so we don’t need infinite penalties.\n",
    "* **Hammer (penalty $\\rho$)**: stabilizes the Hessian by adding positive curvature.\n",
    "* **Key miracle**: we can keep $\\rho$ finite.\n",
    "\n",
    "## 3. Convergence Theorem (with Intuition)\n",
    "\n",
    "**Theorem (Rockafellar-type)**:\n",
    "If LICQ and SOSC hold, and we update\n",
    "\n",
    "$$\n",
    "x^{k+1} = \\arg\\min_x \\mathcal{L}_A(x,\\lambda^k,\\rho^k), \\quad \n",
    "\\lambda^{k+1} = \\lambda^k + \\rho^k h(x^{k+1}),\n",
    "$$\n",
    "\n",
    "then:\n",
    "\n",
    "* $x^k \\to x^*$ (primal feasibility and optimality)\n",
    "\n",
    "* $\\lambda^k \\to \\lambda^*$ (dual multipliers converge)\n",
    "\n",
    "* $\\rho^k$ can remain bounded\n",
    "\n",
    "**Intuition**:\n",
    "\n",
    "* At each step, multipliers “learn” how much force is needed to enforce the constraint.\n",
    "* The penalty term ensures the local quadratic problem is well-conditioned.\n",
    "* Together, the system damps oscillations and guides us to the saddle point safely.\n",
    "\n",
    "## 4. Why $\\rho$ Stays Finite\n",
    "\n",
    "* In pure penalty methods: $\\rho \\to \\infty$ is the *only* way to kill constraint violations.\n",
    "* Here, the multiplier update:\n",
    "\n",
    "  $$\n",
    "  \\lambda^{k+1} = \\lambda^k + \\rho h(x^{k+1})\n",
    "  $$\n",
    "\n",
    "  automatically amplifies the constraint force.\n",
    "* Thus, $\\rho$ just needs to provide **numerical curvature**, not exact enforcement.\n",
    "\n",
    "**Metaphor**: $\\lambda$ is the brain (learning forces), $\\rho$ is just the muscle tone (stability). You don’t need infinite muscles if you have a brain.\n",
    "\n",
    "## 5. Convergence Rate\n",
    "\n",
    "* **Locally linear**:\n",
    "\n",
    "  $$\n",
    "  \\|x^k - x^*\\| + \\|\\lambda^k - \\lambda^*\\| = O(\\sigma^k), \\quad 0<\\sigma<1\n",
    "  $$\n",
    "* Behaves like Newton’s method on KKT conditions near optimum.\n",
    "* **Penalty tuning**:\n",
    "\n",
    "  * Too small $\\rho$: convergence slows.\n",
    "  * Too large $\\rho$: ill-conditioning.\n",
    "  * Optimal: moderate $\\rho$ gives fastest convergence.\n",
    "\n",
    "## 6. Hessian Conditioning: The Hidden Hero\n",
    "\n",
    "$$\n",
    "\\nabla^2_{xx}\\mathcal{L}_A = \\nabla^2 f(x) + \\sum_i \\lambda_i \\nabla^2 h_i(x) + \\rho \\sum_i \\nabla h_i(x)\\nabla h_i(x)^\\top\n",
    "$$\n",
    "\n",
    "* Last term is **always positive semidefinite**.\n",
    "* Ensures the Hessian is better conditioned → optimization algorithms behave nicely.\n",
    "* Without it, you’re stuck with indefinite Hessians.\n",
    "\n",
    "## 7. Practical Guidelines for Students\n",
    "\n",
    "* **Check feasibility**: $|h(x^k)| \\to 0$ is the most important measure.\n",
    "* **Watch $\\lambda$**: they stabilize once you’re near optimum.\n",
    "* **Don’t over-crank $\\rho$**: the method works with moderate penalties.\n",
    "* **Compare methods**:\n",
    "\n",
    "  * Penalty → slow, ill-conditioned.\n",
    "  * SQP → fast (quadratic) but costly.\n",
    "  * Augmented Lagrangian → sweet spot: robust, simple, efficient.\n",
    "\n",
    "✅ **Pedagogical Takeaway:** Augmented Lagrangian works because multipliers “do the enforcing” while penalties “do the conditioning.” This allows convergence to the exact solution with a finite, stable penalty parameter — something neither pure penalty nor pure Lagrangian methods can achieve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeab805",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
